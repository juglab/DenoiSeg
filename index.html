<!DOCTYPE html>
<html lang="en">
<head>
    <script src="http://www.google.com/jsapi"></script>
    <script >google.load("jquery", "1.3.2");</script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <style>
        body {
            font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
            font-weight:300;
            font-size:18px;
            margin-left: auto;
            margin-right: auto;
            width: 1100px;
        }

        h1 {
            font-weight:300;
        }

        .disclaimerbox {
            background-color: #eee;		
            border: 1px solid #eeeeee;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
            padding: 20px;
        }

        video.header-vid {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }

        img.header-img {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }

        img.rounded {
            border: 1px solid #eeeeee;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }

        a:link,a:visited {
            color: #1367a7;
            text-decoration: none;
        }
        
        a:hover {
            color: #208799;
        }

        td.dl-link {
            height: 160px;
            text-align: center;
            font-size: 22px;
        }

        .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
            margin-left: 10px;
            margin-right: 45px;
        }

        .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

            margin-left: 10px;
            margin-right: 45px;
        }


        .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
            margin-top: 5px;
            margin-left: 10px;
            margin-right: 30px;
            margin-bottom: 5px;
        }

        .vert-cent {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }

        footer {
            background-color: #424242;
            border-top: thin solid #424242;
            color: #ccc;
            font-size: 0.75rem;
            font-weight: 300;
            padding: 0.5rem;
            position: fixed;
            left: 0px;
            bottom: 0px;
            width: 100%;
        }
        
        .wrapper{
            margin: auto;
            width: 283px;
        }
        
        hr {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
        }
        
        p {
            text-align: justify;
        }
    </style>

    <title>DenoiSeg: Joint Denoising and Segmentation</title>
    <meta property="og:image" content="./resources/Teaser_1.png"/>
    <meta property="og:title" content="DenoiSeg: Joint Denoising and Segmentation" />
</head>

<body>
<br />
<center><span style="font-size:42px">DenoiSeg: Joint Denoising and Segmentation</span></center>
<table align=center width=800px>
    <tr>
        <td align=center width=600px>
            <center>
                <span style="font-size:23px"><a href="https://tibuch.github.io/">Tim-Oliver Buchholz<sup>*</sup></a></span>
            </center>
        </td>
        <td align=center width=600px>
            <center>
                <span style="font-size:23px"><a href="https://twitter.com/Mangal_Prakash_">Mangal Prakash<sup>*</sup></a></span>
            </center>
        </td>
        <td align=center width=400px>
            <center>
                <span style="font-size:23px"><a href="https://alex-krull.github.io/aboutMe/">Alexander Krull</a></span>
            </center>
        </td>
        <td align=center width=400px>
            <center>
                <span style="font-size:23px"><a href="https://www.mpi-cbg.de/research-groups/current-groups/florian-jug/group-leader/">Florian Jug</a></span>
            </center>
        </td>
</table>

<table align=center width=700px>
    <tr>
        <td align=center width=100px>
            <center>
                <span style="font-size:15px"><sup>*</sup> Equal Contribution (Alphabetical order)</span>
            </center>
        </td>
    </tr>
</table>

<table align=center width=650px>
    <tr>
        <td align=center width=150px>
            <center>
                <span style="font-size:23px"><a href='https://github.com/juglab/DenoiSeg'>[GitHub]</a></span>
            </center>
        </td>
        <!--<td align=center width=150px>
        <center>
        <span style="font-size:23px"><a href=''>[Slides]</a></span>
        </center>
        </td> -->
        <td align=center width=150px>
            <center>
                <span style="font-size:23px"><a href='https://arxiv.org/abs/2005.02987'>[Paper]</a></span>
            </center>
        </td>
    </tr>
</table>
<br>
<p>
    Microscopy image analysis often requires the segmentation of objects, but training data for this task is typically scarce and hard to obtain.
    Here we propose DenoiSeg, a new method that can be trained end-to-end on only a few annotated ground truth segmentations.
    We achieve this by extending <a href="https://github.com/juglab/n2v">Noise2Void</a>, a self-supervised denoising scheme that can be trained on noisy images alone, to also predict dense
    3-class segmentations. The reason for the success of our method is that segmentation can profit from denoising, especially when performed jointly within the same network.
    The network becomes a denoising expert by seeing all available raw data, while  co-learning to segment, even if only a few segmentation labels are available.
    This hypothesis is additionally fueled by our observation that the best segmentation results on high quality (very low noise) raw data are obtained when moderate amounts of synthetic noise are added. We believe that DenoiSeg offers a viable way to circumvent the tremendous hunger for high quality training data and effectively enables few-shot learning of dense segmentations.
</p>

<br /><br />

<table align="center" width="1100">
    <tr>
        <td>
            <img src = "./resources/images/Teaser_1.png" alt="DenoiSeg Teaser" width="1100" />
        </td>
    </tr>
    <tr>
        <td width=400px>
            <p>
                <b>The proposed DenoiSeg training scheme. A U-Net is trained with a joint self-supervised denoising loss and a classical segmentation loss. Both losses are weighted with respect to each other by a hyperparameter $\alpha$. In this example, $\mathcal{L_{d}}$ can be computed on all 3800 training patches, while $\mathcal{L_{s}}$ can only be computed on the 10 available annotated ground truth patches that are available for segmentation. Some results are shown in the <a href="#qualitative_results">Qualitative Results</a> Section.</b>
            </p>
        </td>
    </tr>
</table>

<br /><br />

<hr>

<div class="disclaimerbox">
    <div style="color:#646464">
        <div style="font-size:28px; text-align: center;"><b>How to use DenoiSeg</b></div>
        <p>
            <i>Welcome! We believe our work is a significant step forward in solving the problem of biomedical image segmentation when not many annoated ground truth images are available for training Deep Learning networks. We provide our results on three diverse datasets which can be downloaded through our notebooks <a href="https://github.com/juglab/DenoiSeg/tree/master/examples/DenoiSeg_2D">here</a>. Some qualitative segmentation results with DenoiSeg can be found <a href="#qualitative_results">below</a>. For more detailed results, please refer to DenoiSeg <a href="https://github.com/juglab/DenoiSeg/wiki"> wiki</a>. Please enjoy our results, and if you're so inclined, <a href="https://github.com/juglab/DenoiSeg/tree/master/examples/DenoiSeg_2D">try the notebooks</a> yourself!</i>
        </p>
    </div>
</div>

<br /><br />

<hr>

<center><h1 id="paper">Paper and Supplementary Material</h1></center>
<table align=center width=625px>
    <tr>
        <td>
            <a href="https://arxiv.org/abs/2005.02987"><img class="layered-paper-big" style="height:195px" src="./resources/images/Paper.png" /></a>
        </td>
        <td>
            <span style="font-size:14pt">
                Buchholz<sup>*</sup>, Prakash<sup>*</sup>, Krull, Jug.<br>
                DenoiSeg: Joint Denoising and Segmentation.<br>
                Preprint, 2020.<br>
                <a href="./resources/bibtex.txt">Bibtex</a>
            </span>
        </td>
    </tr>
</table>

<br /><br />

<hr>

<center><h1 id="qualitative_results">Qualitative Results</h1></center>
<p>
    We show results on three diverse datasets: <a href="https://www.kaggle.com/c/data-science-bowl-2018">DSB 2018</a>, a developing Fly Wing and a Mouse Nuclei dataset.
    For each dataset, we add Gaussian noise with zero mean and standard deviation 10 and 20.
    The dataset names are accordingly extended by n0, n10 and n20 to indicate the amount of additional noise.
    We compare against two baselines: (i) DenoiSeg trained purely for segmentation (referred to as Baseline), and a sequential scheme based on <a href="https://arxiv.org/pdf/1911.12239.pdf">Prakash <i>et al.</i></a> that first trains a denoiser and then the aforementioned baseline (referred to as Sequential).
</p>
<table align=center>
    <tr>
        <td>
            <center><img src = "./resources/images/qualitative_n20_fraction1.png" alt="Qualitative Results" /></center>
        </td>
    </tr>
    <tr>
        <td>
            <p align=justify>
                <b>Qualitative results on DSB n10 (first row), Fly Wing n10 (second row) and
                Mouse Nuclei n10 (third row). The first column shows an example test image. Numbers
                indicate how many noisy input and annotated ground truth (GT) patches were used
                for training. Note that segmentation GT was only available for at most 10 images,
                accounting for less than 0.27% of the available raw data. Other columns show depicted
                inset regions, from left to right showing: raw input, segmentation GT, results of two
                baseline methods, and our DenoiSeg segmentation and denoising results.<br />
                Furhter results can be found on the <a href="https://github.com/juglab/DenoiSeg/wiki">DenoiSeg Wiki</a>.</b>
            </p>
        </td>
    </tr>
</table>

<br /><br />

<hr>

<center><h1 id="perform_comp">Quantitative Performance Comparisons</h1></center>
<p>
    We compare the results of DenoiSeg with the above mentioned baselines in terms of <a href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf">Average Precision (AP)</a> score and
    <a href="https://www.nature.com/articles/nmeth.4473">SEG-Score</a>. We evaluated DenoiSeg setups with different
    values of hyperparameter $\alpha$. The figure below presents an evaluation of the DenoiSeg performance for $\alpha=0.5$ across three different noise levels. Furhter results can be found on the <a href="https://github.com/juglab/DenoiSeg/wiki">DenoiSeg Wiki</a>.
</p>
<table align="center" width="1100">
    <tr>
        <td>
            <img src = "./resources/images/quantitative_DSB.png" alt="Quantitative Results" width = "1100" />
        </td>
    </tr>
    <tr>
        <td>
            <p align=justify>
                <b>Results for DSB n0, n10 and n20, evaluated with <a href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf">Average Precision (AP)</a> and <a href="https://www.nature.com/articles/nmeth.4473">SEG-Score</a>. DenoiSeg outperforms both baseline methods, mainly when only limited segmentation ground truth is available. Note that the advantage of our proposed method is at least partially compromised when the image data is not noisy (row 3).</b>
            </p>
        </td>
    </tr>
</table>

<br /><br />

<hr>

<center><h1 id="related_work">Recent Related Work</h1></center>
<p>
    There have been some recent works which leverage denoising for image segmentation when not enough segmentation ground truth is available for training deep learning networks. 			  We would like to direct you to these recent related work for comparison. For a more thorough discussion of related work, please see our
    <a href="https://arxiv.org/abs/2005.02987">full paper</a>.
</p>
<ul>
    <li>
        Mangal Prakash, Tim-Oliver Buchholz, Manan Lalit, Pavel Tomancak, Florian Jug<sup>*</sup>, Alexander Krull<sup>*</sup>. <b>Leveraging Self-Supervised Denoising for Image Segmentation.</b> In <i>ISBI</i>, 2020. <a href="https://arxiv.org/pdf/1911.12239.pdf">[PDF]</a>
    </li>
    <li>
        Sicheng Wang, Bihan Wen, Junru Wu, Dacheng Tao, Zhangyang Wang. <b>Segmentation-Aware Image Denoising without Knowing True Segmentation.</b> May 2019. <a href="https://arxiv.org/pdf/1905.08965.pdf">[PDF]</a>
    </li>
</ul>

<br /><br />

<hr>

<center><h1 id="acknowledgements">Acknowledgements</h1></center>
<p>
    We thank Romina Piscitello-Gomez and Suzanne Eaton from MPI-CBG for fly wing data and Diana Afonso and Jacqueline Tabler from MPI-CBG for mouse nuclei data.
    We also acknowledge the Scientific Computing Facility at MPI-CBG for giving us access to their HPC cluster.
</p>

<br><br>

<footer>
    <div class="wrapper">
        &copy; Copyright 2020 <a href="https://github.com/juglab">Jug-Lab</a>.
        Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.
    </div>
</footer>
</body>
</html>
