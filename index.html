<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>DenoiSeg: Joint Denoising and Segmentation</title>
		<meta property="og:image" content="https://juglab.github.io/DenoiSeg/resources/Teaser_1.png"/>
		<meta property="og:title" content="DenoiSeg: Joint Denoising and Segmentation" />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">DenoiSeg: Joint Denoising and Segmentation</span>
	  		  <table align=center width=800px>
	  			  <tr>
	  	              <td align=center width=600px>
	  					<center>
	  						<span style="font-size:23px"><a href="https://tibuch.github.io/">Tim-Oliver Buchholz<sup>*</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=600px>
	  					<center>
	  						<span style="font-size:23px"><a href="">Mangal Prakash<sup>*</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=400px>
	  					<center>
	  						<span style="font-size:23px"><a href="https://alex-krull.github.io/aboutMe/">Alexander Krull</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=400px>
	  					<center>
	  						<span style="font-size:23px"><a href="https://www.mpi-cbg.de/research-groups/current-groups/florian-jug/group-leader/">Florian Jug</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
			  
			  <table align=center width=700px>
			         <tr>
			          <td align=center width=100px>
			          <center>
			            <span style="font-size:15px"><sup>*</sup> Equal Contribution (Alphabetical order)</span>
			          </center>
				  </td>
			       </tr>
			      </table>

	  		  <table align=center width=650px>
	  			  <tr>
	  	              
	  	              <td align=center width=150px>
	  					<center>
	  						<span style="font-size:23px"><a href='https://github.com/juglab/DenoiSeg'> [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=150px>
	  					<center>
	  						<span style="font-size:23px"><a href=''> [Slides]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=150px>
	  					<center>
	  						<span style="font-size:23px"><a href='https://arxiv.org/abs/2005.02987'> [Paper]</a></span>
		  		  		</center>
		  		  	  </td>

		  		  	 </tr>
	  			  <tr>
			  </table>
  		  <br>
  		  <table align=center width=850px>
  			  <tr>
  	              <td width=100px>
  					<center>
  	                	<a href=""><img class="rounded" src = "./resources/images/Teaser_1.png" height="175px"></img></href></a><br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					<p align="justify">
  	                	<span style="font-size:14px"><i>The proposed DenoiSeg training scheme. A U-Net is trained with a joint self-supervised denoising loss and a 						classical segmentation loss. Both losses are weighted with respect to each other by a hyperparameter $\alpha$. In this example, 
						$\mathcal{L_{d}}$ can be computed on all 3800 training patches, while 
						$\mathcal{L_{s}}$ can only be computed on the 10 available annotated ground truth patches that are available for segmentation. To check the performance, see the 
						<a href="#qualitative_results"> <b> Qualitative results</b></a> section.</i>
					</p>
  	              </td>

  		  </table>

      	  <br>
		  <hr>
			<div class="disclaimerbox">
	  		  <!-- <center><h2>How to interpret the results</h2></center> -->

	  		 <span style="color:#646464">
	  		  	<center><span style="font-size:28px"><b>How to use DenoiSeg</b></span></center>

	  		  	<br>
				<p align="justify">
				<i> Welcome! We believe our work is a significant step forward in solving the problem of biomedical image segmentation when not many annoated ground truth images are available for training Deep Learning networks. We provide our results on three diverse datasets which can be downloaded through our notebooks <a href="https://github.com/juglab/DenoiSeg/tree/master/examples/DenoiSeg_2D">here </a>. Some qualitative segmentation results with DenoiSeg can be found <a href="#qualitative_results">below</a>. For more detailed results, please refer to DenoiSeg <a href="https://github.com/juglab/DenoiSeg/wiki"> wiki</a>. Please enjoy our results, and if you're so inclined, <a href="https://github.com/juglab/DenoiSeg/tree/master/examples/DenoiSeg_2D">try the notebooks</a> yourself</i>!
			</p>
				
			</span>

			</div>
  		  <br><br>
		  <hr>


  		  <table align=center width=850px>
	  		  <center><h1>Abstract</h1></center>
	  		  <tr>
	  		  	<td>
<!-- 					Given a grayscale photograph as input, this paper attacks the problem of hallucinating a <i>plausible</i> color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and explore using class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward operation in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test", asking human subjects to choose between a generated and ground truth color image. Our method successfully fools humans 20% of the time, significantly higher than previous methods. -->
	  		    </td>
	  		  </tr>
			</table>
					<p align="justify">
					Microscopy image analysis often requires the segmentation of objects, but training data for this task is typically scarce and hard to obtain.
					Here we propose DenoiSeg, a new method that can be trained end-to-end on only a few annotated ground truth segmentations. 
					We achieve this by extending Noise2Void, a self-supervised denoising scheme that can be trained on noisy images alone, to also predict dense  					
					3-class segmentations. The reason for the success of our method is that segmentation can profit from denoising, especially when performed jointly within the same network.
					The network becomes a denoising expert by seeing all available raw data, while  co-learning to segment, even if only a few segmentation labels are available.
					This hypothesis is additionally fueled by our observation that the best segmentation results on high quality (very low noise) raw data are obtained when moderate amounts 					of synthetic noise are added. We believe that DenoiSeg offers a viable way to circumvent the tremendous hunger for high quality training data and effectively enables 					few-shot learning of dense segmentations.
				</p>
					
  		  <br><br>
		  <hr>

  		  <!-- <table align=center width=550px> -->
  		  <table align=center width=425px>
	 		<center><h1>Paper and Supplementary Material</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!--a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <td><a href=""><img class="layered-paper-big" style="height:195px" src="./resources/images/Paper.png"/></a></td>
				  <td><span style="font-size:14pt">Buchholz<sup>*</sup>, Prakash<sup>*</sup>, Krull, Jug.<br>
				  DenoiSeg: Joint Denoising and Segmentation.<br>
				  In MICCAI, 2020.<br>
				  (hosted on <a href="https://arxiv.org/abs/2005.02987">arXiv</a>)</a>
				  <span style="font-size:4pt"><a href=""><br></a>
				  </span>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <!-- <br> -->
<!--   		  <table align=center width=200px>
			  <tr>
				  <td><span style="font-size:11pt"><a href="http://arxiv.org/pdf/1603.08511v1.pdf">Previous version [v1] [10MB]</a></td>
				  <td><span style="font-size:12pt"><a href="./resources/supp.pdf">Additional details [v1] [1MB]</a></td>
			  </tr>
			  <tr>
				  <td><span style="font-size:11pt"><a href="./resources/supp.pdf">Additional details [v1] [1MB]</a></td>
			  </tr>
		  </table> -->

		  <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="./resources/bibtex.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>

		  <hr>

  		  <a name="qualitative_results"></a>
  		  <center><h1>Qualitative Results</h1></center>
		  <p align="justify">
  		  We show results on three diverse datasets: DSB 2018, a developing Fly Wing and a Mouse Nuclei dataset. 
		  For each dataset, we add Gaussian noise with mean 0 and standard deviation 10 and 20. 
		  The dataset names are accordingly extended by n0, n10 and n20 to indicate the amount of additional noise.
		  We compare against two baselines: (i) DenoiSeg trained purely for segmentation (referred to as Baseline), and a sequential scheme based on <a 			href="https://arxiv.org/pdf/1911.12239.pdf">Prakash et. al </a> that first trains a denoiser and then the aforementioned baseline (referred to as Sequential).
		 </p>
		  <br><br>
		  
		  <b>The figure below depicts results for noise level n20 when only 10, 2 and 2 ground truth segmentation annotations for DSB, Fly Wing and Mouse Nuclei dataset 			respectively are available for training</b>. For many more qualitative results, please see DenoiSeg <a href="https://github.com/juglab/DenoiSeg/wiki">wiki</a>. 
		  
		  <br>

    		  <!-- along with miscellaneous photos. -->
    		  <br>
    		  <table align=center width=1100px>
    			  <tr>
    	              <td width=1100px>
    					<center>
    						<span style="font-size:22px"><a href='./resources/images/qualitative_n20_fraction1.png'></a></span><br>
    	                	<a href="./resources/images/qualitative_n20_fraction1.png"><img src = "./resources/images/qualitative_n20_fraction1.png" height = "600px"></a><br>
    					<span style="font-size:14px"></span><br>
    					<span style="font-size:14px">extension of Figure 2 from our paper</span></center>
    	              </td>

                  </tr>
    		  </table>
			
    		  <br>
  		   
			
    	  	<hr>

  		  <a name="perform_comp"></a>
  		  <center><h1>Quantitative Performance Comparisons</h1></center>
		  <p align="justify">
		  
		  We compare the results of DenoiSeg with the above mentioned baselines in terms of <a href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf"> AP</a> score and 
		  <a href="https://www.nature.com/articles/nmeth.4473"> SEG</a> score. We evaluated DenoiSeg setups with different 
		  values of hyperparameter $\alpha$.
		  For DSB dataset with noise levels n0, n10 and n20, the figure below presents comparison of DenoiSeg setup with $\alpha=0.5$ with respect to the above mentioend baselines. 
		  For more results and comparisons with different datasets using different values of hyperparameter $\alpha$, please see DenoiSeg 
		  <a href="https://github.com/juglab/DenoiSeg/wiki">wiki</a>.
		  
		  <br>

    		  <!-- along with miscellaneous photos. -->
    		  <br>
    		  <table align=center width=1100px>
    			  <tr>
    	              <td width=1100px>
    					<center>
    						<span style="font-size:22px"><a href='./resources/images/quantitative_DSB.png'></a></span><br>
    	                	<a href="./resources/images/qualitative_n20_fraction1.png"><img src = "./resources/images/quantitative_DSB.png" height = "600px"></a><br>
    					<span style="font-size:14px"></span><br>
    					<span style="font-size:14px">Figure 3 from our paper</span></center>
    	              </td>

                  </tr>
    		  </table>
			
    		  <br>
		  
		  
		 <br>
		 

		<hr>
		<br>

  		  <a name="related_work"></a>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  					
	  		  <center><h1>Recent Related Work</h1></center>
			  
			  <p align="justify">

	  		  There have been some recent works which leverage denoising for image segmentation when not enough segmentation ground truth is available for training deep learning networks. 			  We would like to direct you to these recent related work for comparison. For a more thorough discussion of related work, please see our 
			  <a href="https://arxiv.org/abs/2005.02987">full paper</a>.

	  		  <br><br>

	  		  		<!-- <span style="font-size:24px"><center><b> Previous Work </b> </center></span><br> -->
					Mangal Prakash, Tim-Oliver Buchholz, Manan Lalit, Pavel Tomancak, Florian Jug<sup>*</sup>, Alexander Krull<sup>*</sup>. 
					<b>Leveraging Self-Supervised Denoising for Image Segmentation.</b> In <i>ISBI</i>, 2020. <a href="https://arxiv.org/pdf/1911.12239.pdf">[PDF]</a><br>

					Sicheng Wang, Bihan Wen, Junru Wu, Dacheng Tao, Zhangyang Wang. <b>Segmentation-Aware Image Denoising without Knowing True Segmentation.</b> May 2019. <a href="https://arxiv.org/pdf/1905.08965.pdf">[PDF]</a><br>
					
				</p>

					<!-- Ding Liu, Bihan Wen, Xianming Liu, Zhangyang Wang, Thomas S. Huang. <b>When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach.</b> In <i>IJCAI</i>, 2018. <a href="https://www.ijcai.org/Proceedings/2018/0117.pdf">[PDF]</a><br> -->
					
			   		
				</td>
			 </tr>
		 </table>

		  <br>
		  <hr>
		  <br>
		  	
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  					
	  		  <center><h1>Acknowledgements</h1></center>
			  
			  <p align="justify">
			  We thank Romina Piscitello-Gomez and Suzanne Eaton from MPI-CBG for fly wing data and Diana Afonso and Jacqueline Tabler from MPI-CBG for mouse nuclei data. 
			  We also acknowledge the Scientific Computing Facility at MPI-CBG for giving us access to their HPC cluster.
		  </p>

		</td>
			 </tr>
		</table>

		<br><br>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-1', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 